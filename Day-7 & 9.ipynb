{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Day-7**"
      ],
      "metadata": {
        "id": "kq3BLl_yoXgf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import spacy\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Download stopwords if not already downloaded\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Load spaCy English model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # Convert text to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Tokenize using spaCy\n",
        "    doc = nlp(text)\n",
        "\n",
        "    # Get English stopwords from NLTK\n",
        "    stop_words = set(stopwords.words(\"english\"))\n",
        "\n",
        "    # Remove stopwords and punctuation\n",
        "    filtered_words = [token.text for token in doc if token.text not in stop_words and token.is_alpha]\n",
        "\n",
        "    return \" \".join(filtered_words)\n",
        "\n",
        "# Example usage\n",
        "text = \"This is a Sample Text! It contains stopwords and punctuation.\"\n",
        "processed_text = preprocess_text(text)\n",
        "print(\"Processed Text:\", processed_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PcbD1rC-oPUv",
        "outputId": "af8b2a87-8bc0-48fe-a902-621c1969c78b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed Text: sample text contains stopwords punctuation\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Day-9**"
      ],
      "metadata": {
        "id": "I5HDChgOoow_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim\n",
        "import nltk\n",
        "from gensim.utils import simple_preprocess\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Download necessary NLTK resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Initialize stemmer and lemmatizer\n",
        "stemmer = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # Convert text to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Tokenization using Gensim\n",
        "    tokens = simple_preprocess(text)\n",
        "\n",
        "    # Remove stopwords\n",
        "    stop_words = set(stopwords.words(\"english\"))\n",
        "    filtered_tokens = [word for word in tokens if word not in stop_words]\n",
        "\n",
        "    # Apply stemming\n",
        "    stemmed_words = [stemmer.stem(word) for word in filtered_tokens]\n",
        "\n",
        "    # Apply lemmatization\n",
        "    lemmatized_words = [lemmatizer.lemmatize(word, wordnet.VERB) for word in filtered_tokens]\n",
        "\n",
        "    return {\n",
        "        \"original_tokens\": tokens,\n",
        "        \"filtered_tokens\": filtered_tokens,\n",
        "        \"stemmed_words\": stemmed_words,\n",
        "        \"lemmatized_words\": lemmatized_words\n",
        "    }\n",
        "\n",
        "# Read sample text from a file\n",
        "file_path = \"sample.txt\"  # Change this to your actual file path\n",
        "try:\n",
        "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
        "        text = file.read()\n",
        "\n",
        "    processed_data = preprocess_text(text)\n",
        "\n",
        "    # Print results\n",
        "    print(\"Original Tokens:\", processed_data[\"original_tokens\"])\n",
        "    print(\"Filtered Tokens (No Stopwords):\", processed_data[\"filtered_tokens\"])\n",
        "    print(\"Stemmed Words:\", processed_data[\"stemmed_words\"])\n",
        "    print(\"Lemmatized Words:\", processed_data[\"lemmatized_words\"])\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: The file was not found. Please check the file path.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bW_uXXwvovS_",
        "outputId": "0228f2a9-cf76-4836-e322-06ff6b38b6e2"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: The file was not found. Please check the file path.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        }
      ]
    }
  ]
}